{
 "metadata": {
  "name": "",
  "signature": "sha256:d024f07e6bdf1cffda3dd39e0767cb25c61eeddc8853107047da6edbd018b30a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Introduction to Decision Trees\n",
      "\n",
      "*Adapted from Chapter 8 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Motivation\n",
      "\n",
      "Why are we learning about decision trees?\n",
      "\n",
      "- They're useful for both regression and classification problems.\n",
      "- They're popular (for a variety of reasons).\n",
      "- They're the basis for more sophisticated modeling approaches.\n",
      "- They demonstrate a different way of \"thinking\" than the models we have studied so far."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Regression trees\n",
      "\n",
      "Let's look at a simple example to get started.\n",
      "\n",
      "Our goal is to **predict a baseball player's Salary** based on **Years** (number of years playing in the major leagues) and **Hits** (number of hits he made in the previous year). Here is the training data, represented visually (low salary is blue/green, high salary is red/yellow):"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Salary data](images/salary_color.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**How might you \"stratify\" or \"segment\" the feature space into regions, based on salary?** Here are the rules:\n",
      "\n",
      "- You can only use straight lines, drawn one at a time.\n",
      "- Your line must either be vertical or horizontal.\n",
      "- Your line stops when it hits an existing line.\n",
      "\n",
      "Intuitively, you want to **maximize** the similarity (or \"homogeneity\") within a given region, and **minimize** the similarity between different regions.\n",
      "\n",
      "*Let's take a minute and do this...*\n",
      "\n",
      "Below is a regression tree that has been fit to the data by a computer. (We will talk later about how the fitting algorithm actually works.) Note that  Salary is measured in thousands and has been log-transformed."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Salary tree](images/salary_tree.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**How do we make Salary predictions (for out-of-sample data) using a decision tree?**\n",
      "\n",
      "- Start at the top, and examine the first \"splitting rule\" (Years < 4.5).\n",
      "- If the rule is **True** for a given player, follow the **left branch**. If the rule is **False**, follow the **right branch**.\n",
      "- Continue until reaching the bottom. The predicted Salary is the number in that particular \"bucket\".\n",
      "- **Note:** Years and Hits are both integers, but the convention is to label these rules using the midpoint between adjacent values.\n",
      "\n",
      "Example predictions:\n",
      "\n",
      "- Years=3, then predict 5.11 ($\\$1000 \\times e^{5.11} \\approx \\$166000$)\n",
      "- Years=5 and Hits=100, then predict 6.00 ($\\$1000 \\times e^{6.00} \\approx \\$403000$)\n",
      "- Years=8 and Hits=120, then predict 6.74 ($\\$1000 \\times e^{6.74} \\approx \\$846000$)\n",
      "\n",
      "**How did we come up with the numbers at the bottom of the tree?** Each number is just the **mean Salary in the training data** of players who fit that criteria.\n",
      "\n",
      "Here's the same diagram as before, split into the three regions:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Salary regions](images/salary_regions.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This diagram is essentially a combination of the two previous diagrams. In $R_1$, the mean log Salary was 5.11. In $R_2$, the mean log Salary was 6.00. In $R_3$, the mean log Salary was 6.74. Thus, those values are used to predict out-of-sample data.\n",
      "\n",
      "Let's introduce some terminology:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Salary tree annotated](images/salary_tree_annotated.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**How might you interpret the \"meaning\" of this tree?**\n",
      "\n",
      "- Years is the most important factor determining Salary, with a lower number of Years corresponding to a lower Salary.\n",
      "- For a player with a lower number of Years, Hits is not an important factor determining Salary.\n",
      "- For a player with a higher number of Years, Hits is an important factor determining Salary, with a greater number of Hits corresponding to a higher Salary.\n",
      "\n",
      "What we have seen so far hints at the advantages and disadvantages of decision trees:\n",
      "\n",
      "**Advantages:**\n",
      "\n",
      "- Highly interpretable\n",
      "- Can be displayed graphically\n",
      "- Prediction is fast\n",
      "\n",
      "**Disadvantages:**\n",
      "\n",
      "- Predictive accuracy is not as high as some supervised learning methods\n",
      "- Can easily overfit the training data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Building a regression tree by hand\n",
      "\n",
      "How do you build a decision tree? You're going to find out by building one in pairs!\n",
      "\n",
      "Your training data is a tiny dataset of [used vehicle sale prices](https://raw.githubusercontent.com/justmarkham/DAT5/master/data/vehicles_train.csv). Your goal is to predict **price** for testing data. Here are your instructions:\n",
      "\n",
      "- Read the data into Pandas.\n",
      "- Explore the data by sorting, plotting, or split-apply-combine (aka `group_by`).\n",
      "- Decide which feature is the most important predictor, and use that to make your first split. (Only binary splits are allowed!)\n",
      "- After making your first split, you should actually split your data in Pandas into two parts, and then explore each part to figure out what other splits to make.\n",
      "- Stop making splits once you are convinced that it strikes a good balance between underfitting and overfitting. (As always, your goal is to build a model that generalizes well!)\n",
      "- You are allowed to split on the same variable multiple times!\n",
      "- Draw your tree, making sure to label your leaves with the mean Price for the observations in that \"bucket\".\n",
      "- When you're finished, review your tree to make sure nothing is backwards. (Remember: follow the **left branch** if the rule is **true**, and follow the **right branch** if the rule is **false**.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## How does a computer build a regression tree?\n",
      "\n",
      "The ideal approach would be for the computer to consider every possible partition of the feature space. However, this is computationally infeasible, so instead an approach is used called **recursive binary splitting:**\n",
      "\n",
      "- Begin at the top of the tree.\n",
      "- For every single predictor, examine every possible cutpoint, and choose the predictor and cutpoint such that the resulting tree has the **lowest possible mean squared error (MSE)**. Make that split.\n",
      "- Repeat the examination for the two resulting regions, and again make a single split (in one of the regions) to minimize the MSE.\n",
      "- Keep repeating this process until a stopping criterion is met, such as **maximum tree depth** or **minimum number of samples in a leaf**.\n",
      "\n",
      "Below is the regression tree for player salaries grown much deeper, and a comparison of the training, test, and cross-validation errors for trees with different numbers of leaves:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Salary unpruned](images/salary_unpruned.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see, the **training error** continues to go down as the tree size increases, but the lowest **cross-validation error** occurs for a tree with 3 leaves."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Building a regression tree in scikit-learn"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read the training data into pandas and print it out\n",
      "import pandas as pd\n",
      "data = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT5/master/data/vehicles_train.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# encode car as 0 and truck as 1\n",
      "data['kind'] = data.type.map({'car':0, 'truck':1})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a list of the feature columns (every column except for the 0th colum\n",
      "feat_cols = ['year', 'miles', 'doors', 'kind']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define X (features) and y (response)\n",
      "X = data[feat_cols]\n",
      "y = data.price"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import the relevant class, and instantiate the model (with random_state=1)\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "treereg = DecisionTreeRegressor(random_state=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print the model object to see the default arguments\n",
      "treereg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "DecisionTreeRegressor(compute_importances=None, criterion='mse',\n",
        "           max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "           min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "           random_state=1, splitter='best')"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use 3-fold cross-validation to estimate the RMSE for this model\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "import numpy as np\n",
      "scores = cross_val_score(tree, X, y, cv=3, scoring = 'mean_squared_error')\n",
      "\n",
      "from sklearn import cross_validation\n",
      "scores = cross_validation.cross_val_score(treereg, X, y, cv=3, scoring = 'mean_squared_error')\n",
      "np.mean(np.sqrt(-scores))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "4707.2505884845632"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Tuning a regression tree\n",
      "\n",
      "Let's see if we can reduce the RMSE by tuning the **max_depth** parameter. One way to search for an optimal value would be to try different values, one by one:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# try max_depth=1\n",
      "treereg = DecisionTreeRegressor(max_depth=1, random_state=1)\n",
      "scores = cross_val_score(treereg, X, y, cv=3, scoring='mean_squared_error')\n",
      "np.mean(np.sqrt(-scores))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "4928.1374642038018"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Or, we could write a loop to try a range of values:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define a range of values\n",
      "max_depth_range = range(1, 11)\n",
      "\n",
      "# create an empty list to store the average RMSE for each value of max_depth\n",
      "RMSE_scores = []\n",
      "\n",
      "# use cross-validation with each value of max_depth\n",
      "for depth in max_depth_range:\n",
      "    treereg = DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
      "    MSE_scores = cross_val_score(treereg, X, y, cv=3, scoring='mean_squared_error')\n",
      "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
      "\n",
      "# print the results\n",
      "RMSE_scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plot the max_depth (x-axis) versus the RMSE (y-axis)\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "plt.plot(max_depth_range, RMSE_scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In a later class, we'll learn how to use scikit-learn's [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) to locate these optimal parameters more easily."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# max_depth=3 was best, so fit a tree using that parameter\n",
      "treereg = DecisionTreeRegressor(max_depth=3, random_state=1)\n",
      "treereg.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the \"Gini importance\" of each feature: the (normalized) total reduction of MSE brought by that feature\n",
      "pd.DataFrame({'feature':feature_cols, 'importance':treereg.feature_importances_})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Creating a tree diagram"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a Graphviz file\n",
      "from sklearn.tree import export_graphviz\n",
      "with open(\"tree_vehicles.dot\", 'wb') as f:\n",
      "    f = export_graphviz(treereg, out_file=f, feature_names=feature_cols)\n",
      "\n",
      "# At the command line, run this to convert to PNG:\n",
      "# dot -Tpng tree_vehicles.dot -o tree_vehicles.png"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Tree for vehicle data](images/tree_vehicles.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How do we read this decision tree?\n",
      "\n",
      "**Internal nodes:**\n",
      "\n",
      "- \"samples\" is the number of observations in that node before splitting\n",
      "- \"mse\" is the mean squared error calculated by comparing the actual response values in that node against the mean response value in that node\n",
      "- first line is the condition used to split that node (go left if true, go right if false)\n",
      "\n",
      "**Leaves:**\n",
      "\n",
      "- \"samples\" is the number of observations in that node\n",
      "- \"value\" is the mean response value in that node\n",
      "- \"mse\" is the mean squared error calculated by comparing the actual response values in that node against \"value\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Predicting on test data\n",
      "\n",
      "How good is scikit-learn's regression tree at predicting the price for test observations?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read the test data\n",
      "test = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT5/master/data/vehicles_test.csv')\n",
      "\n",
      "# encode car as 0 and truck as 1\n",
      "test['type'] = test.type.map({'car':0, 'truck':1})\n",
      "\n",
      "# print the data\n",
      "test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define X and y\n",
      "X_test = test[feature_cols]\n",
      "y_test = test.price"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# make predictions on test data\n",
      "y_pred = treereg.predict(X_test)\n",
      "y_pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate RMSE\n",
      "from sklearn import metrics\n",
      "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate RMSE for your own tree!\n",
      "y_test = [3000, 6000, 12000]\n",
      "y_pred = [3057, 3057, 16333]\n",
      "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Classification trees\n",
      "\n",
      "Classification trees are very similar to regression trees. Here is a quick comparison:\n",
      "\n",
      "|regression trees|classification trees|\n",
      "|---|---|\n",
      "|predict a continuous response|predict a categorical response|\n",
      "|predict using mean response of each leaf|predict using most commonly occuring class of each leaf|\n",
      "|splits are chosen to minimize MSE|splits are chosen to minimize Gini index (discussed below)|\n",
      "\n",
      "Here's an **example of a classification tree**, which predicts whether Barack Obama or Hillary Clinton would win the Democratic primary in a particular county in 2008:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Obama-Clinton decision tree](images/obama_clinton_tree.jpg)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**A few questions:**\n",
      "\n",
      "- What is the response variable?\n",
      "- What are the features?\n",
      "- What is the most predictive feature?\n",
      "- How would we calculate the total number of counties?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Splitting criteria for classification trees\n",
      "\n",
      "Here are common options for the splitting criteria:\n",
      "\n",
      "- **classification error rate:** fraction of training observations in a region that don't belong to the most common class\n",
      "- **Gini index:** measure of total variance across classes in a region\n",
      "- **cross-entropy:** numerically similar to Gini index\n",
      "\n",
      "The goal when splitting is to increase the \"node purity\", and it turns out that the **Gini index and cross-entropy** are better measures of purity than classification error rate. The Gini index is faster to compute than cross-entropy, so it is generally preferred (and is used by scikit-learn by default)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Example calculations of Gini index\n",
      "\n",
      "Let's say that we are predicting survival on the Titanic. At a particular node, there are **25 individuals**, of whom 10 survived and 15 died. Here's how we calculate the Gini index before making a split:\n",
      "\n",
      "$$1 - \\left(\\frac {Survived} {Total}\\right)^2 - \\left(\\frac {Died} {Total}\\right)^2 = 1 - \\left(\\frac {10} {25}\\right)^2 - \\left(\\frac {15} {25}\\right)^2 = 0.48$$\n",
      "\n",
      "The **maximum value** of the Gini index is 0.5, and occurs when the classes are perfectly balanced in a node. The **minimum value** of the Gini index is 0, and occurs when there is only one class represented in a node. Thus, a node with a lower Gini index is said to be more \"pure\".\n",
      "\n",
      "**When deciding between splits**, the decision tree algorithm chooses the split that maximizes the resulting node purity. Let's pretend that gender was the split being considered, and the resulting nodes are as follows:\n",
      "\n",
      "- **Males:** 2 survived, 13 died\n",
      "- **Females:** 8 survived, 2 died\n",
      "\n",
      "To evaluate this split, we calculate the **weighted average of the Gini indices of the resulting nodes:**\n",
      "\n",
      "$$\\text{Males: } 1 - \\left(\\frac {2} {15}\\right)^2 - \\left(\\frac {13} {15}\\right)^2 = 0.23$$\n",
      "$$\\text{Females: } 1 - \\left(\\frac {8} {10}\\right)^2 - \\left(\\frac {2} {10}\\right)^2 = 0.32$$\n",
      "$$\\text{Weighted Average: } 0.23 \\left(\\frac {15} {25}\\right) + 0.32 \\left(\\frac {10} {25}\\right) = 0.27$$\n",
      "\n",
      "Thus, the decrease in Gini index (and gain in purity) from splitting on gender is **0.21**. The decision tree algorithm will choose this split if no other splits result in a larger gain in purity."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Building a classification tree in scikit-learn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read in the data\n",
      "titanic = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT5/master/data/titanic_train.csv')\n",
      "titanic.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's choose our response variable and a few features, and review **how to handle categorical features**:\n",
      "\n",
      "- **Survived:** This is our response variable, and is already encoded as 0=died and 1=survived.\n",
      "- **Pclass:** These are the passenger class categories (1=first class, 2=second class, 3=third class). They are logically ordered, so we'll leave them as-is. (If the tree splits on this feature, the splits will occur at 1.5 or 2.5.)\n",
      "- **Sex:** This is a binary category, so we should encode it as 0=female and 1=male. (If the tree splits on this feature, the split will occur at 0.5.)\n",
      "- **Age:** This is a numeric feature, but we need to fill in the missing values.\n",
      "- **Embarked:** This is the port they embarked from. There are three unordered categories, so we should create dummy variables and drop one level as usual."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# encode female as 0 and male as 1\n",
      "titanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\n",
      "\n",
      "# fill in the missing values for age with the mean age\n",
      "titanic.Age.fillna(titanic.Age.mean(), inplace=True)\n",
      "\n",
      "# create three dummy variables, drop the first dummy variable, and store the two remaining columns as a DataFrame\n",
      "embarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked').iloc[:, 1:]\n",
      "\n",
      "# concatenate the two dummy variable columns onto the original DataFrame\n",
      "titanic = pd.concat([titanic, embarked_dummies], axis=1)\n",
      "\n",
      "# print the updated DataFrame\n",
      "titanic.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a list of the feature columns\n",
      "feature_cols = ['Pclass', 'Sex', 'Age', 'Embarked_Q', 'Embarked_S']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define X (features) and y (response)\n",
      "X = titanic[feature_cols]\n",
      "y = titanic.Survived"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# fit a classification tree with max_depth=3 on all data\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
      "treeclf.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a Graphviz file\n",
      "with open(\"tree_titanic.dot\", 'wb') as f:\n",
      "    f = export_graphviz(treeclf, out_file=f, feature_names=feature_cols)\n",
      "\n",
      "# At the command line, run this to convert to PNG:\n",
      "# dot -Tpng tree_titanic.dot -o tree_titanic.png"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Tree for Titanic data](images/tree_titanic.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice the split in the bottom right. The **same class** is predicted in both of its leaves! Why did this split occur?\n",
      "\n",
      "Although that split didn't affect the **classification error rate**, it did increase the **node purity**. Node purity is important because we're interested in the class proportions among the observations in each region."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the feature importances\n",
      "pd.DataFrame({'feature':feature_cols, 'importance':treeclf.feature_importances_})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Wrapping up decision trees\n",
      "\n",
      "Here are some advantages and disadvantages of decision trees that we haven't yet talked about:\n",
      "\n",
      "**Advantages:**\n",
      "\n",
      "- Can be specified as a series of rules, and are thought to more closely approximate human decision-making than other models\n",
      "- Non-parametric (will do better than linear models if relationship between features and response is highly non-linear)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Trees versus linear models](images/tree_vs_linear.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Disadvantages:**\n",
      "\n",
      "- Small variations in the data can result in a completely different tree (high variance)\n",
      "- Recursive binary splitting makes \"locally optimal\" decisions that may not result in a globally optimal tree\n",
      "- Can create biased trees if the classes are highly imbalanced\n",
      "\n",
      "Note that there is not just one decision tree algorithm; instead, there are many variations. A few common decision tree algorithms that are often referred to by name are C4.5, C5.0, and CART. (More details are available in the [scikit-learn documentation](http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart).) scikit-learn uses an \"optimized version\" of CART."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Resources\n",
      "\n",
      "- scikit-learn documentation: [Decision Trees](http://scikit-learn.org/stable/modules/tree.html)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}